{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification \n",
    "- This is a comprehensive tutorial covering intermediate (and advanced) topics in text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/anaconda/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# feature engineering (words to vectors)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# classification algorithms (or classifiers)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "# build a pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# for gridsearch ... buiild many models with different parameters (e.g., with/without bi-gram)\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# model evaluation, validation\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score, train_test_split \n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Loading the dataset\n",
    "\n",
    "The data is in a single csv file. We can use Pandas, a python package, to load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_data = []\n",
    "sms_labels = []\n",
    "sms = []\n",
    "\n",
    "openfile = open('data/spam.csv', 'rb')\n",
    "r = csv.reader(openfile)\n",
    "for i in r:\n",
    "    sms.append(i)\n",
    "    sms_data.append(i[0])\n",
    "    sms_labels.append(i[1])    \n",
    "openfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corpus (in a csv file) contains spam emails and regular emails. Two **labels** or **y values** are 0 (regular email) and 1 (spam email). \n",
    "\n",
    "We'll **train a predictive (or machine learning) model to learn to discriminate between spam emails and regular emails automatically**. Then, **the predictive model will be used to classify a set of unknown (or unlabeled) emails as either spam or regular**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5574\n",
      "5574\n",
      "5574\n"
     ]
    }
   ],
   "source": [
    "#entire data\n",
    "print len(sms)\n",
    "#texts only\n",
    "print len(sms_data)\n",
    "#labels only\n",
    "print len(sms_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...',\n",
       " '0']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first row\n",
    "sms[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4827\n",
      "747\n"
     ]
    }
   ],
   "source": [
    "# how many spams in the dataset\n",
    "print sms_labels.count('0')\n",
    "print sms_labels.count('1')  #1 refers to spam emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "747 spam emails & 4827 regular emails in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Preprocessing & Step 3: Feature Engineering (Words to Vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5574, 8713)\n",
      "  (0, 3571)\t0.147879386867\n",
      "  (0, 8084)\t0.230018923239\n",
      "  (0, 4374)\t0.326467645747\n",
      "  (0, 5958)\t0.255351135361\n",
      "  (0, 2338)\t0.252829676565\n",
      "  (0, 1316)\t0.244190457021\n",
      "  (0, 5571)\t0.156034537655\n",
      "  (0, 4114)\t0.107000554123\n",
      "  (0, 1767)\t0.275803221504\n",
      "  (0, 3655)\t0.180346776875\n",
      "  (0, 8548)\t0.220834221079\n",
      "  (0, 4501)\t0.275803221504\n",
      "  (0, 1765)\t0.311649251596\n",
      "  (0, 2061)\t0.275803221504\n",
      "  (0, 7694)\t0.155520993044\n",
      "  (0, 3615)\t0.1530562397\n",
      "  (0, 1082)\t0.326467645747\n",
      "  (0, 8324)\t0.182416008163\n",
      "  (1, 5538)\t0.271904356737\n",
      "  (1, 4537)\t0.40832852092\n",
      "  (1, 4342)\t0.523676940648\n",
      "  (1, 8450)\t0.43163099771\n",
      "  (1, 5567)\t0.546619596648\n",
      "  (2, 4114)\t0.0790778808417\n",
      "  (2, 3373)\t0.113114957061\n",
      "  :\t:\n",
      "  (5572, 4245)\t0.122078808454\n",
      "  (5572, 8371)\t0.187304281983\n",
      "  (5572, 1097)\t0.112250676563\n",
      "  (5572, 4642)\t0.159548788302\n",
      "  (5572, 7089)\t0.184334185597\n",
      "  (5572, 3323)\t0.121464328313\n",
      "  (5572, 7674)\t0.102211246877\n",
      "  (5572, 1451)\t0.142906084668\n",
      "  (5572, 5367)\t0.210111258217\n",
      "  (5572, 2606)\t0.184334185597\n",
      "  (5572, 8120)\t0.208882217995\n",
      "  (5572, 1794)\t0.136336592679\n",
      "  (5572, 7099)\t0.205416471364\n",
      "  (5572, 2905)\t0.2440936565\n",
      "  (5572, 3489)\t0.275370505634\n",
      "  (5572, 1802)\t0.283015445641\n",
      "  (5572, 3709)\t0.242599467234\n",
      "  (5572, 4188)\t0.283015445641\n",
      "  (5572, 914)\t0.324869215164\n",
      "  (5572, 1561)\t0.340316196196\n",
      "  (5573, 7806)\t0.148538709688\n",
      "  (5573, 5276)\t0.390908096649\n",
      "  (5573, 4253)\t0.578547159472\n",
      "  (5573, 7938)\t0.423587102037\n",
      "  (5573, 6548)\t0.55765963002\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(decode_error ='ignore')\n",
    "tfidf = tfidf_vectorizer.fit_transform(sms_data)\n",
    "print tfidf.shape\n",
    "print tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'00',\n",
       " u'000',\n",
       " u'000pes',\n",
       " u'008704050406',\n",
       " u'0089',\n",
       " u'0121',\n",
       " u'01223585236',\n",
       " u'01223585334',\n",
       " u'0125698789',\n",
       " u'02']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.get_feature_names()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'zhong',\n",
       " u'zindgi',\n",
       " u'zoe',\n",
       " u'zogtorius',\n",
       " u'zoom',\n",
       " u'zouk',\n",
       " u'zyada',\n",
       " u'\\xe8n',\n",
       " u'\\xfa1',\n",
       " u'\\u3028ud']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.get_feature_names()[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Term Matrix (DTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix = tfidf_vectorizer.fit_transform(sms_data).toarray()\n",
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000pes</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>02</th>\n",
       "      <th>...</th>\n",
       "      <th>zhong</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtorius</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zyada</th>\n",
       "      <th>èn</th>\n",
       "      <th>ú1</th>\n",
       "      <th>〨ud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ok lar... Joking wif u oni...</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U dun say so early hor... U c already then say...</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nah I don't think he goes to usf, he lives around here though</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8713 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     00  000  000pes  \\\n",
       "Go until jurong point, crazy.. Available only i...  0.0  0.0     0.0   \n",
       "Ok lar... Joking wif u oni...                       0.0  0.0     0.0   \n",
       "Free entry in 2 a wkly comp to win FA Cup final...  0.0  0.0     0.0   \n",
       "U dun say so early hor... U c already then say...   0.0  0.0     0.0   \n",
       "Nah I don't think he goes to usf, he lives arou...  0.0  0.0     0.0   \n",
       "\n",
       "                                                    008704050406  0089  0121  \\\n",
       "Go until jurong point, crazy.. Available only i...           0.0   0.0   0.0   \n",
       "Ok lar... Joking wif u oni...                                0.0   0.0   0.0   \n",
       "Free entry in 2 a wkly comp to win FA Cup final...           0.0   0.0   0.0   \n",
       "U dun say so early hor... U c already then say...            0.0   0.0   0.0   \n",
       "Nah I don't think he goes to usf, he lives arou...           0.0   0.0   0.0   \n",
       "\n",
       "                                                    01223585236  01223585334  \\\n",
       "Go until jurong point, crazy.. Available only i...          0.0          0.0   \n",
       "Ok lar... Joking wif u oni...                               0.0          0.0   \n",
       "Free entry in 2 a wkly comp to win FA Cup final...          0.0          0.0   \n",
       "U dun say so early hor... U c already then say...           0.0          0.0   \n",
       "Nah I don't think he goes to usf, he lives arou...          0.0          0.0   \n",
       "\n",
       "                                                    0125698789   02 ...   \\\n",
       "Go until jurong point, crazy.. Available only i...         0.0  0.0 ...    \n",
       "Ok lar... Joking wif u oni...                              0.0  0.0 ...    \n",
       "Free entry in 2 a wkly comp to win FA Cup final...         0.0  0.0 ...    \n",
       "U dun say so early hor... U c already then say...          0.0  0.0 ...    \n",
       "Nah I don't think he goes to usf, he lives arou...         0.0  0.0 ...    \n",
       "\n",
       "                                                    zhong  zindgi  zoe  \\\n",
       "Go until jurong point, crazy.. Available only i...    0.0     0.0  0.0   \n",
       "Ok lar... Joking wif u oni...                         0.0     0.0  0.0   \n",
       "Free entry in 2 a wkly comp to win FA Cup final...    0.0     0.0  0.0   \n",
       "U dun say so early hor... U c already then say...     0.0     0.0  0.0   \n",
       "Nah I don't think he goes to usf, he lives arou...    0.0     0.0  0.0   \n",
       "\n",
       "                                                    zogtorius  zoom  zouk  \\\n",
       "Go until jurong point, crazy.. Available only i...        0.0   0.0   0.0   \n",
       "Ok lar... Joking wif u oni...                             0.0   0.0   0.0   \n",
       "Free entry in 2 a wkly comp to win FA Cup final...        0.0   0.0   0.0   \n",
       "U dun say so early hor... U c already then say...         0.0   0.0   0.0   \n",
       "Nah I don't think he goes to usf, he lives arou...        0.0   0.0   0.0   \n",
       "\n",
       "                                                    zyada   èn   ú1  〨ud  \n",
       "Go until jurong point, crazy.. Available only i...    0.0  0.0  0.0  0.0  \n",
       "Ok lar... Joking wif u oni...                         0.0  0.0  0.0  0.0  \n",
       "Free entry in 2 a wkly comp to win FA Cup final...    0.0  0.0  0.0  0.0  \n",
       "U dun say so early hor... U c already then say...     0.0  0.0  0.0  0.0  \n",
       "Nah I don't think he goes to usf, he lives arou...    0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 8713 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Document Term Matrix\n",
    "pd.DataFrame(tfidf_matrix,index=sms_data,columns=tfidf_vectorizer.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 Model Building / Model Validation / Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our features, we can train a classifier to try to predict the category of a post. Let’s start with a naïve Bayes classifier, which provides a nice baseline for this task. scikit-learn includes several variants of this classifier; the one most suitable for word counts is the multinomial variant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97613921779691426"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the below model does not use \"split validation\", meaning this model is NOT validated. \n",
    "# We create this invalidated model to illustrate the concep called \"overfitting\"\n",
    "# overfitting happends that the model fits extremely well into the existing dataset, but would not be suitable for new datasets\n",
    "# Simply, the model is not much generalizable.\n",
    "\n",
    "nb = MultinomialNB().fit(tfidf, sms_labels)\n",
    "\n",
    "predicted = nb.predict(tfidf)\n",
    "np.mean(predicted == sms_labels)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " overall accuracy:\n",
      "0.976139217797\n",
      "\n",
      " confusion_matrix:\n",
      "[[4827    0]\n",
      " [ 133  614]]\n",
      "\n",
      " Here is the classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.99      4827\n",
      "          1       1.00      0.82      0.90       747\n",
      "\n",
      "avg / total       0.98      0.98      0.98      5574\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print '\\n overall accuracy:'\n",
    "print metrics.accuracy_score(sms_labels, predicted)\n",
    "\n",
    "print '\\n confusion_matrix:'\n",
    "print metrics.confusion_matrix(sms_labels, predicted)\n",
    "\n",
    "print '\\n Here is the classification report:'\n",
    "print metrics.classification_report(sms_labels, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 4827 regular mails correctly predicted as regular & zero false positive \n",
    "- 614 spam correctly as spam & 133 false negative ==> This indicates that you would expect some spam mails in your inbox.\n",
    "- About different measures (e.g., prediction) for model accuracy, https://en.wikipedia.org/wiki/Precision_and_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using scikit-plot python package (https://github.com/reiinakano/scikit-plot) ... you need to install \n",
    "# !pip install scikit-plot\n",
    "from scikitplot import plotters as skplt\n",
    "\n",
    "skplt.plot_confusion_matrix(y_true=sms_labels, y_pred=predicted)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true positive rate = 614 / 747 = 82%\n",
    "# false positive rate = 0 / 4827 = 0%\n",
    "\n",
    "probas = nb.predict_proba(tfidf)\n",
    "skplt.plot_roc_curve(y_true=sms_labels, y_probas=probas)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above ROC curve shows our Naive Bayesian classification model is almost perfect. \n",
    "<br><br>\n",
    "The diagonal line represents pure guessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split validation\n",
    "\n",
    "In Step 4, we built a model using different machine learning (or classification) algorithms. However, it is a **bad idea to evaluate the performance of the model on the same dataset we train the model on**. Thus, we'll use a validation method called ** split validation**.\n",
    "\n",
    "First, we split the dataset to two parts: **training dataset (70% of the original dataset)** and **testing dataset (30% of the original dataset)**. We build a model using training dataset and apply the model to testing dataset and measure the accuracy of the model. You could have a 80-20 split or a 50-50 split.\n",
    "\n",
    "We will build a predictive model using **x_train** and **y_train**, which are called as **training dataset**.\n",
    "\n",
    "Then, we will apply the model to **x_test** and **y_test** (**testing dataset**) and this will tell us the performance or quality of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"http://www.developer.com/imagesvr_ce/6793/ML4.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(sms_data, sms_labels, test_size=0.3, random_state=0)\n",
    "len(x_train), len(y_train), len(x_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tfidf_vectorizer.fit_transform(x_train)\n",
    "X_test = tfidf_vectorizer.transform(x_test)\n",
    "print X_train[:2]\n",
    "print \"+++++++++++++++++++++++++++++++++++++++++\"\n",
    "print X_test[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "predicted = nb.predict(X_test)\n",
    "np.mean(predicted == y_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print '\\n overall accuracy:'\n",
    "print metrics.accuracy_score(y_test, predicted)\n",
    "\n",
    "print '\\n confusion_matrix:'\n",
    "print metrics.confusion_matrix(y_test, predicted)\n",
    "\n",
    "print '\\n Here is the classification report:'\n",
    "print metrics.classification_report(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- true positive rate is only 69%\n",
    "- false negative rate is very high, meaning a large number of spams will be found in your inbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using scikit-plot python package (https://github.com/reiinakano/scikit-plot) ... you need to install \n",
    "# !pip install scikit-plot\n",
    "from scikitplot import plotters as skplt\n",
    "\n",
    "skplt.plot_confusion_matrix(y_true=np.array(y_test), y_pred=predicted)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of split validation:\n",
    "* The accuracy of Naive Bayes-based model on this dataset was about 97%. This performance is based on when the predictive model was tested on the same dataset on which the model was built. **again, this is a bad practice since the model appears to be better than what it is**. This is called **\"model overfitting**.\n",
    "\n",
    "* The **true accuracy** of Naive Bayes-based model on this dataset turns out to be about 95%, according to **split validation**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Model Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_new = [\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question\",\n",
    "            \"Even my brother is not like to speak with me. They treat me like aids patent.\",\n",
    "             \"As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9\",\n",
    "            \"hello, thank you\",\n",
    "           \"To claim txt DIS to 87121\"]\n",
    "\n",
    "unlabeled_tfidf = tfidf_vectorizer.transform(docs_new)\n",
    "\n",
    "predicted = nb.predict(unlabeled_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's find the probability\n",
    "\n",
    "predicted_prob = nb.predict_proba(unlabeled_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted_prob):\n",
    "    print('%r => %s' % (doc, category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendixes\n",
    "- We need to understand the following topics as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix. Building a pipeline\n",
    "\n",
    "As you have seen, building a classification (or predictive) model requires several steps. **Pipeline** is what chains these different steps together, and thus streamline the predictive modeling process. Using pipeline we write less codes.\n",
    "\n",
    "Also, Pipeline can build several predictive models through a loop to find the best one.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline\n",
    "http://scikit-learn.org/stable/modules/pipeline.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining feature engineering and model building\n",
    "nb_pipeline = Pipeline([('tfidf', TfidfVectorizer(decode_error ='ignore')), ('clf', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_pipeline = nb_pipeline.fit(x_train, y_train)\n",
    "nb_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = nb_pipeline.predict(x_test)\n",
    "np.mean(predicted == y_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a complete pipeline\n",
    "\n",
    "nb_pipeline = Pipeline([('tfidf', TfidfVectorizer(decode_error ='ignore')), ('clf', MultinomialNB())])\n",
    "nb_pipeline = nb_pipeline.fit(x_train, y_train)\n",
    "predicted = nb_pipeline.predict(x_test)\n",
    "np.mean(predicted == y_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix. Other Classifiers: How to find a better model with different classifiers or classification algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://scikit-learn.org/stable/_static/ml_map.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNeighbors Classifier (kNN)\n",
    "- http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "- http://scikit-learn.org/stable/modules/neighbors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knn_pipeline = Pipeline([('tfidf', TfidfVectorizer(decode_error ='ignore')), ('clf', KNeighborsClassifier())])\n",
    "knn_pipeline = knn_pipeline.fit(x_train, y_train)\n",
    "predicted = knn_pipeline.predict(x_test)\n",
    "np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)\n",
    "- http://scikit-learn.org/stable/modules/svm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_pipeline = Pipeline([('tfidf', TfidfVectorizer(decode_error ='ignore')), ('clf', SVC(kernel='linear', probability=True))])\n",
    "svm_pipeline = svm_pipeline.fit(x_train, y_train)\n",
    "predicted = svm_pipeline.predict(x_test)\n",
    "np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that SVM works best for this dataset. You can use this svm model for model deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to figure out processing time\n",
    "\n",
    "import timeit\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "svm_pipeline = Pipeline([('tfidf', TfidfVectorizer(decode_error ='ignore')), ('clf', SVC(kernel='linear', probability=True))])\n",
    "svm_pipeline = svm_pipeline.fit(x_train, y_train)\n",
    "predicted = svm_pipeline.predict(x_test)\n",
    "print np.mean(predicted == y_test)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print \"%s %s\" % (\"this processing has taken\", elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my computer the above processing took about 2.63 seconds. How about yours?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_new = [\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question\",\n",
    "            \"Even my brother is not like to speak with me. They treat me like aids patent.\",\n",
    "             \"As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9\",\n",
    "            \"hello, thank you\",\n",
    "           \"To claim txt DIS to 87121\"]\n",
    "\n",
    "#unlabeled_tfidf = tfidf_vectorizer.transform(docs_new)\n",
    "predicted = svm_pipeline.predict(docs_new)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's find the probability\n",
    "\n",
    "predicted_prob = svm_pipeline.predict_proba(docs_new)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted_prob):\n",
    "    print('%s => %s' % (doc, (category)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix. Changing Parameters: How to improve this model? How to find a better model? How to deploy the predictive model in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving the Naive Bayes-based predictive model by changing parameter values?\n",
    "\n",
    "See the parameters in Naive Bayes classifier\n",
    "\n",
    "- TfidfVectorizer(input=’content’, encoding=’utf-8’, decode_error=’strict’, strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, analyzer=’word’, stop_words=None, token_pattern=’(?u)\\b\\w\\w+\\b’, ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class ‘numpy.int64’>, norm=’l2’, use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "- max_df : float in range [0.0, 1.0] or int, default=1.0 \n",
    "When building the vocabulary ignore terms that have a **document frequency** strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. \n",
    "    - For example, max_df = 0.7 ==> This removes words which appear in more than 70% of the corpus (**removing frequent words**).\n",
    "<br><br>    \n",
    "- min_df : float in range [0.0, 1.0] or int, default=1\n",
    "When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. \n",
    "    - For example, min_df = 5 ==> This removes words which appear in less than five documents (**removing rare words**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using different parameter values (e.g., removing stopwords, using stemming words, using ngrams, removing too frequent words, removing too rare words), the model accuracy can be improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stopwords\n",
    "\n",
    "nb_pipeline = Pipeline([('tfidf', TfidfVectorizer(decode_error ='ignore', stop_words='english')), ('clf', MultinomialNB())])\n",
    "nb_pipeline = nb_pipeline.fit(x_train, y_train)\n",
    "predicted = nb_pipeline.predict(x_test)\n",
    "np.mean(predicted == y_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stopwords & rare words\n",
    "\n",
    "nb_pipeline = Pipeline([('tfidf', TfidfVectorizer(decode_error ='ignore', stop_words='english', min_df=2)), ('clf', MultinomialNB())])\n",
    "nb_pipeline = nb_pipeline.fit(x_train, y_train)\n",
    "predicted = nb_pipeline.predict(x_test)\n",
    "np.mean(predicted == y_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stopwords\n",
    "\n",
    "knn_pipeline = Pipeline([('tfidf', TfidfVectorizer(decode_error ='ignore', stop_words='english')), ('clf', KNeighborsClassifier())])\n",
    "knn_pipeline = knn_pipeline.fit(x_train, y_train)\n",
    "predicted = knn_pipeline.predict(x_test)\n",
    "np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stopwords\n",
    "\n",
    "svm_pipeline = Pipeline([('tfidf', TfidfVectorizer(decode_error ='ignore', stop_words='english')), ('clf', SVC(kernel='linear', probability=True))])\n",
    "svm_pipeline = svm_pipeline.fit(x_train, y_train)\n",
    "predicted = svm_pipeline.predict(x_test)\n",
    "np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stopwords & using bigram as well\n",
    "\n",
    "knn_pipeline = Pipeline([('tfidf', TfidfVectorizer(decode_error ='ignore', stop_words='english', ngram_range=(1, 2))), ('clf', KNeighborsClassifier())])\n",
    "knn_pipeline = knn_pipeline.fit(x_train, y_train)\n",
    "predicted = knn_pipeline.predict(x_test)\n",
    "np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_pipeline = Pipeline([('tfidf', TfidfVectorizer(decode_error ='ignore', stop_words='english', ngram_range=(1, 2))), ('clf', SVC(kernel='linear', probability=True))])\n",
    "svm_pipeline = svm_pipeline.fit(x_train, y_train)\n",
    "predicted = svm_pipeline.predict(x_test)\n",
    "np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the best model uses **TFIDF (and bigram) with SVM after removing stopwords**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline with multiple classifiers (or algorithms)\n",
    "- Then, can we test the accuracy of all classifiers at once?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifiers = [MultinomialNB(), KNeighborsClassifier(), SVC(kernel='linear', probability=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf in classifiers:\n",
    "    pipeline = Pipeline([('tfidf', TfidfVectorizer(decode_error ='ignore', stop_words='english', ngram_range=(1, 2))), \n",
    "                         ('clf', clf)])\n",
    "    classifiers_pipeline = pipeline.fit(x_train, y_train)\n",
    "    predicted = classifiers_pipeline.predict(x_test)\n",
    "    print \"%s --> %s\" % (clf, np.mean(predicted == y_test))\n",
    "    print \" ++++++++++++++++++++++++++++++++++++++++++++++++++++++++ \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix. k-fold Cross validation (CV) \n",
    "\n",
    "### 10-fold CV\n",
    "* The basic idea is that, rather than testing the model quality **only once**, cross validation (or 10-fold CV) tests the model **10 times** with 10 different testing datasets.\n",
    "\n",
    "#### How?\n",
    "* The training (or original) dataset is randomly partitioned into 10 equal sized subsamples (see the figure below). \n",
    "* At each time, one subsample is set aside as the **testing** or **validation dataset** and the other 9 subsamples are used as the training dataset. \n",
    "* A model is built using the training dataset and tested with the testing dataset. This is done 10 times. \n",
    "* This leads to 10 evaluation scores (mean squared error). The final score is based on the average of the scores.\n",
    "\n",
    "<img src=\"https://chrisjmccormick.files.wordpress.com/2013/07/10_fold_cv.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_pipeline = Pipeline([('tfidf', TfidfVectorizer(decode_error ='ignore', stop_words='english')), ('clf', MultinomialNB())])\n",
    "nb_pipeline = nb_pipeline.fit(sms_data, sms_labels)\n",
    "\n",
    "scores = cross_val_score(nb_pipeline, sms_data, sms_labels, scoring='accuracy', cv=10)\n",
    "print scores\n",
    "print scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knn_pipeline = Pipeline([('tfidf', TfidfVectorizer(decode_error ='ignore', stop_words='english')), ('clf', KNeighborsClassifier())])\n",
    "knn_pipeline = knn_pipeline.fit(sms_data, sms_labels)\n",
    "\n",
    "scores = cross_val_score(knn_pipeline, sms_data, sms_labels, scoring='accuracy', cv=10)\n",
    "print scores\n",
    "print scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "svm_pipeline = Pipeline([('tfidf', TfidfVectorizer(decode_error ='ignore', stop_words='english', ngram_range=(1, 2))), ('clf', SVC(kernel='linear', probability=True))])\n",
    "svm_pipeline = svm_pipeline.fit(sms_data, sms_labels)\n",
    "\n",
    "scores = cross_val_score(svm_pipeline, sms_data, sms_labels, scoring='accuracy', cv=10)\n",
    "print scores\n",
    "print scores.mean()\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print \"%s %s\" % (\"this processing has taken\", elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process has taken 72 seconds in my machine. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix. Parameter tuning using Grid Search (Requiring high computational power!!!) \n",
    "## Warning: This process is likely to slow down your computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\" We’ve already encountered some parameters such as stopwords and ngram in the TfidfTransformer. Classifiers tend to have many parameters as well; e.g., MultinomialNB includes a smoothing parameter alpha and SGDClassifier has a penalty parameter alpha and configurable loss and penalty terms in the objective function (see the module documentation, or use the Python help function, to get a description of these).\n",
    "\n",
    "Instead of tweaking the parameters of the various components of the chain, it is possible to run an exhaustive search of the best parameters on a grid of possible values. We try out all classifiers on either words or bigrams, with or without idf, and with a penalty parameter of either 0.01 or 0.001 for the linear SVM:\"\n",
    "\n",
    "http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_pipeline = Pipeline([('tfidf', TfidfVectorizer(decode_error ='ignore', stop_words='english')), ('clf', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "              'tfidf__min_df': [1, 2, 5],\n",
    "              'tfidf__max_df': [0.9, 0.8, 0.7],\n",
    "              'clf__alpha': (0.01, 0.001, 1),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TfidfVectorizer** \n",
    "\n",
    "- ngram_range : tuple (min_n, max_n)\n",
    "    - The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used.\n",
    "\n",
    "- min_df : float in range [0.0, 1.0] or int, default=1\n",
    "    - When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "**MultinomialNB()** MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n",
    "\n",
    "- alpha : float, optional (default=1.0)\n",
    "    - Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(nb_pipeline, parameters)\n",
    "gs_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the best model recommended by gridsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split validation\n",
    "\n",
    "nb_pipeline = Pipeline([('tfidf', TfidfVectorizer(decode_error='ignore', stop_words='english', min_df=1, \n",
    "                                                  ngram_range=(1,2), max_df=0.9)), \n",
    "                        ('clf', MultinomialNB(alpha=0.01))])\n",
    "nb_pipeline = nb_pipeline.fit(x_train, y_train)\n",
    "predicted = nb_pipeline.predict(x_test)\n",
    "np.mean(predicted == y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 fold cross validation\n",
    "\n",
    "nb_pipeline = Pipeline([('tfidf', TfidfVectorizer(decode_error='ignore', stop_words='english', min_df=1, \n",
    "                                                  ngram_range=(1,2), max_df=0.9)), \n",
    "                        ('clf', MultinomialNB(alpha=0.01))])\n",
    "nb_pipeline = nb_pipeline.fit(sms_data, sms_labels)\n",
    "\n",
    "scores = cross_val_score(nb_pipeline, sms_data, sms_labels, scoring='accuracy', cv=10)\n",
    "print scores\n",
    "print scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix get most informative features for scikit-learn classifier (Naive Bayes) for different class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we know the parameters for the best NB model, let's initialize TfidfVectorizer again\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(decode_error='ignore', stop_words='english', min_df=1, ngram_range=(1,2), max_df=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split validation again\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(sms_data, sms_labels, test_size=0.2, random_state=0)\n",
    "len(x_train), len(y_train), len(x_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform traing data and testing data to tfidf format \n",
    "\n",
    "X_train = tfidf_vectorizer.fit_transform(x_train)\n",
    "X_test = tfidf_vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have more vectors due to bi-grams\n",
    "print X_train.shape\n",
    "print X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print tfidf_vectorizer.get_feature_names()[0:5]\n",
    "print tfidf_vectorizer.get_feature_names()[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "predicted = nb.predict(X_test)\n",
    "np.mean(predicted == y_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = nb.predict_proba(X_test)\n",
    "skplt.plot_roc_curve(y_true=y_test, y_probas=probas)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/26976362/how-to-get-most-informative-features-for-scikit-learn-classifier-for-different-c\n",
    "\n",
    "def most_informative_feature_for_binanry_classification(vectorizer, classifier, n=50):\n",
    "    \n",
    "    class_labels = classifier.classes_\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    topn_class1 = sorted(zip(classifier.coef_[0], feature_names))[:n]\n",
    "    topn_class2 = sorted(zip(classifier.coef_[0], feature_names))[-n:]\n",
    "\n",
    "    for coef, feat in topn_class1:\n",
    "        print(class_labels[0], coef, feat)\n",
    "\n",
    "    print \"=========================================\"\n",
    "\n",
    "    for coef, feat in reversed(topn_class2):\n",
    "        print(class_labels[1], coef, feat)\n",
    "\n",
    "\n",
    "most_informative_feature_for_binanry_classification(tfidf_vectorizer, nb, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears **spam** mails tend to contain such terms as **free, claim, prize, reply, ... **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "- http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html (Naive Bayes)\n",
    "- http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html (KNN)\n",
    "- http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html (SVM)\n",
    "- https://stackoverflow.com/questions/26976362/how-to-get-most-informative-features-for-scikit-learn-classifier-for-different-c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
