{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content Analytics - Text Classification\n",
    "\n",
    "**Content Analytics (or Text Mining)** generally refers to a cateogy of data analytics (Machine Learning) techniques and methods focusing on extracting insights from text data. Two types of content analytics are:\n",
    "\n",
    "- Supervised content analytics (or text mining): Text classification\n",
    "- Unsupervised content analytics (or text mining): Lexicon-based sentiment analysis\n",
    "\n",
    "**(Text) Classification** is the task to **train** algorithms (e.g., regression) to classify customer reviews or other types of documents to a pre-determined label (e.g., positive vs. negative, sports vs politics). Some examples of classification tasks are:\n",
    "\n",
    "- Deciding whether a review is positive or negative  \n",
    "- Deciding whether an email is spam or not.\n",
    "- Deciding what the topic of a news article is, from a fixed list of topic areas such as \"sports,\" \"technology,\" and \"politics.\"\n",
    "http://www.nltk.org/book/ch06.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn-images-1.medium.com/max/800/1*ljCBykAJUnvaZcuPYwm4_A.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://cdn-images-1.medium.com/max/800/1*ljCBykAJUnvaZcuPYwm4_A.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business use cases of Text classification\n",
    "\n",
    "- CB Insights' HR news classification algorithms using supervised machine learning (HR news vs. Non-HR news)\n",
    "    - Hiring indicates company growth / company plans & strategy\n",
    "    - Departure of key executives could suggest problems in the company\n",
    "- Categorizing customer reviews (Customer relationship management [CRM] / Voice of the customers [VOC])\n",
    "    - Positive reviews vs. Negative reviews (Supervised sentiment analysis)\n",
    "    - Product-related vs. Service-related\n",
    "- Categorizing news and social media posts (for investing purposes; supply chain management; risk management)\n",
    "    - Positive news vs. Negative news\n",
    "    - Disaster-related vs. Non-disaster\n",
    "    - Fake news vs Non-fake news\n",
    "- Categorizing other types of documents\n",
    "    - Resumes: Qualified workers vs Non-qualified (based on listed skills)\n",
    "    - ...\n",
    "    - https://skillsengine.com/home (SkillsEngine) HR Search API\n",
    "\n",
    "References:\n",
    "- http://www.datasciencecentral.com/profiles/blogs/5-text-classification-case-studies-using-scikit-learn\n",
    "<br>\n",
    "- http://blog.paralleldots.com/text-analytics/text-classification-applications-use-cases/\n",
    "<br>\n",
    "- https://www.cbinsights.com/research/team-blog/human-resources-news-classification-machine-learning/\n",
    "<br>\n",
    "- http://blog.aylien.com/text-analysis-10-business-use-cases-you-may-not/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Algothims (Classifiers) & Process\n",
    "\n",
    "<img src=\"images/machinelearning.gif\">\n",
    "\n",
    "<img src=\"http://www.nltk.org/images/supervised-classification.png\">\n",
    "\n",
    "- **Features** are **tokens** or **words**\n",
    "- Algorithms are **trained**. For example:\n",
    "    - When such words as **amazing** and **best** appears, the review is **positive (95%)**\n",
    "    - When such words as **terrible** and **worst** appears, the review is **negative (95%)**\n",
    "    - Google self-driving car\n",
    "    - Spot\n",
    "\n",
    "We use an important python package:\n",
    "\n",
    "- sklearn (or scikit-learn) http://scikit-learn.org/stable/index.html\n",
    "\n",
    "<img src = \"http://scikit-learn.org/stable/_static/ml_map.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/anaconda/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# feature engineering (words to vectors)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# classification algorithms (or classifiers)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "# build a pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# for gridsearch ... buiild many models with different parameters (e.g., with/without bi-gram)\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# model evaluation, validation\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score, train_test_split \n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first column\n",
    "sms_data = []\n",
    "# second colummn\n",
    "sms_labels = []\n",
    "# both columns\n",
    "sms = []\n",
    "\n",
    "openfile = open('data/sampledata_classification.csv', 'rb')\n",
    "\n",
    "r = csv.reader(openfile)\n",
    "\n",
    "for i in r:\n",
    "    sms.append(i)\n",
    "    sms_data.append(i[0])\n",
    "    sms_labels.append(i[1])\n",
    "    \n",
    "openfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "16\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "#entire data\n",
    "print len(sms)\n",
    "#texts only\n",
    "print len(sms_data)\n",
    "#labels only\n",
    "print len(sms_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "print sms_labels.count('neg')\n",
    "print sms_labels.count('pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I love this sandwich.', 'pos'],\n",
       " ['this is an amazing place!', 'pos'],\n",
       " ['I feel very good about these beers.', 'pos'],\n",
       " ['this is my best work.', 'pos'],\n",
       " ['what an awesome view', 'pos']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view first five rows\n",
    "sms[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love this sandwich.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the first row in the dataset\n",
    "\n",
    "#remember indexing we learning in the python tutorial\n",
    "sms_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I love this sandwich.',\n",
       " 'this is an amazing place!',\n",
       " 'I feel very good about these beers.',\n",
       " 'this is my best work.',\n",
       " 'what an awesome view']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we process this data through text preprocessing (e.g., tokenization, removing stopcwords, lowercase)\n",
    "\n",
    "- 1st document contains 2 valid tokens (love, sandwiche)\n",
    "- 2nd document contains 2 valid tokens (amazing, place)\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pos', 'pos', 'pos', 'pos', 'pos']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_labels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Preprocessing & Step 3: Feature Engineering (Words to Vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "- tokenization\n",
    "- lowercase\n",
    "- removing stopwords\n",
    "- removing short words\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering (Words to Vectors)\n",
    "\n",
    "- turn words into numerical feature vectors (or simply assigning numbers to words)\n",
    "- this is called \"bag of words\"\n",
    "- each word or token is called \"feature\"\n",
    "- consider a typical corpus (or a collection of customer reviews) ... the number of words would be more than 100,000\n",
    "- http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF, IDF, TFIDF\n",
    "\n",
    "- TF: measures how frequently a term appears \n",
    "    - = Number of times the word appears in a document / Total number of words in the document\n",
    "<br><br>    \n",
    "- IDF: measures the relative importance of a word. for example, such words as \"at\" and \"of\" frequently appear, but little important. IDF **weight down such frequent terms while scale up the rare words** \n",
    "    - = log(total number of documents / number of documents containing the word in question)\n",
    "<br><br> \n",
    "- TFIDF: the importance of words or tokens (or features) in a document\n",
    "    - = tf * idf\n",
    "    - tells the importance of words, which is used in classification\n",
    "\n",
    "#### Example:\n",
    "\n",
    "- A document (#1) contains 2 words wherein the word **love** appears **once**.\n",
    "    - **TF** for love is 1 / 2 = 0.5 \n",
    "<br><br> \n",
    "- Now, assume 16 documents and the word \"love\" appears in **only one document**. \n",
    "    - Then, **IDF** is log(16 / 1) = 1.20\n",
    "    - Thus, **Tf-idf** weight is 0.5 x 1.20 = 0.6020 \n",
    "<br><br>  \n",
    "- A document (#2) contains 2 words wherein the word **amazing** appears **once**.\n",
    "    - **TF** for love is 1 / 2 = 0.5 \n",
    "<br><br> \n",
    "- Now, assume 16 documents and the word \"amazing\" appears in **two document**. \n",
    "    - Then, **IDF** is log(16 / 2) = 0.90\n",
    "    - Thus, **Tf-idf** weight is 0.5 x 0.90 = 0.4515 \n",
    "  \n",
    "### Question: \n",
    " \n",
    "Which word (\"love\" or \"amazing\") would be more informative? Answer: **love** (since this word is quite rare, appearing in only one documents and **higher TFIDF**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The goal of using **tfidf** instead of the raw frequencies of occurrence of a token in a given document is **to scale down the impact of tokens that occur very frequently in a given corpus** and that are hence empirically **less informative than features that occur in a small fraction of the training corpus**\"\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 50)\n",
      "  (0, 29)\t0.657675841497\n",
      "  (0, 41)\t0.367321351165\n",
      "  (0, 36)\t0.657675841497\n",
      "  (1, 41)\t0.309738878146\n",
      "  (1, 26)\t0.360547268532\n",
      "  (1, 4)\t0.482966063159\n",
      "  (1, 3)\t0.482966063159\n",
      "  (1, 34)\t0.554576467399\n",
      "  (2, 18)\t0.370774539408\n",
      "  (2, 44)\t0.425750068072\n",
      "  (2, 22)\t0.370774539408\n",
      "  (2, 0)\t0.425750068072\n",
      "  (2, 40)\t0.425750068072\n",
      "  (2, 7)\t0.425750068072\n",
      "  (3, 41)\t0.310470382646\n",
      "  (3, 26)\t0.361398766256\n",
      "  (3, 31)\t0.393675168137\n",
      "  (3, 9)\t0.555886200243\n",
      "  (3, 49)\t0.555886200243\n",
      "  (4, 4)\t0.449213153406\n",
      "  (4, 47)\t0.515818942009\n",
      "  (4, 5)\t0.515818942009\n",
      "  (4, 45)\t0.515818942009\n",
      "  (5, 41)\t0.285432922908\n",
      "  (5, 14)\t0.445066553795\n",
      "  :\t:\n",
      "  (10, 22)\t0.449213153406\n",
      "  (10, 39)\t0.515818942009\n",
      "  (10, 6)\t0.515818942009\n",
      "  (10, 46)\t0.515818942009\n",
      "  (11, 31)\t0.353286140205\n",
      "  (11, 14)\t0.434439844114\n",
      "  (11, 32)\t0.434439844114\n",
      "  (11, 17)\t0.498855162763\n",
      "  (11, 27)\t0.498855162763\n",
      "  (12, 1)\t0.5\n",
      "  (12, 19)\t0.5\n",
      "  (12, 12)\t0.5\n",
      "  (12, 43)\t0.5\n",
      "  (13, 3)\t0.707106781187\n",
      "  (13, 18)\t0.707106781187\n",
      "  (14, 26)\t0.317947896466\n",
      "  (14, 33)\t0.42590266866\n",
      "  (14, 21)\t0.489052162167\n",
      "  (14, 20)\t0.489052162167\n",
      "  (14, 30)\t0.489052162167\n",
      "  (15, 41)\t0.276833035598\n",
      "  (15, 11)\t0.431657020764\n",
      "  (15, 8)\t0.495659724282\n",
      "  (15, 25)\t0.495659724282\n",
      "  (15, 15)\t0.495659724282\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(decode_error ='ignore')\n",
    "tfidf = tfidf_vectorizer.fit_transform(sms_data)\n",
    "print tfidf.shape\n",
    "print tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 17 documents; 77 unique tokens or terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 32)\n",
      "  (0, 22)\t0.707106781187\n",
      "  (0, 25)\t0.707106781187\n",
      "  (1, 1)\t0.65674041915\n",
      "  (1, 23)\t0.754116716334\n",
      "  (2, 13)\t0.548942934718\n",
      "  (2, 17)\t0.548942934718\n",
      "  (2, 4)\t0.630335869871\n",
      "  (3, 6)\t0.707106781187\n",
      "  (3, 31)\t0.707106781187\n",
      "  (4, 2)\t0.707106781187\n",
      "  (4, 30)\t0.707106781187\n",
      "  (5, 21)\t0.707106781187\n",
      "  (5, 24)\t0.707106781187\n",
      "  (6, 28)\t0.707106781187\n",
      "  (6, 26)\t0.707106781187\n",
      "  (7, 9)\t1.0\n",
      "  (8, 27)\t0.707106781187\n",
      "  (8, 11)\t0.707106781187\n",
      "  (9, 7)\t0.707106781187\n",
      "  (9, 18)\t0.707106781187\n",
      "  (10, 17)\t0.65674041915\n",
      "  (10, 3)\t0.754116716334\n",
      "  (11, 12)\t0.707106781187\n",
      "  (11, 20)\t0.707106781187\n",
      "  (12, 0)\t0.5\n",
      "  (12, 14)\t0.5\n",
      "  (12, 8)\t0.5\n",
      "  (12, 29)\t0.5\n",
      "  (13, 1)\t0.707106781187\n",
      "  (13, 13)\t0.707106781187\n",
      "  (14, 16)\t0.707106781187\n",
      "  (14, 15)\t0.707106781187\n",
      "  (15, 5)\t0.57735026919\n",
      "  (15, 19)\t0.57735026919\n",
      "  (15, 10)\t0.57735026919\n"
     ]
    }
   ],
   "source": [
    "# removing stopwords\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(decode_error ='ignore', stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(sms_data)\n",
    "print tfidf.shape\n",
    "print tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'aint',\n",
       " u'amazing',\n",
       " u'awesome',\n",
       " u'beer',\n",
       " u'beers',\n",
       " u'believe',\n",
       " u'best',\n",
       " u'boss',\n",
       " u'dandy',\n",
       " u'deal',\n",
       " u'doing',\n",
       " u'enemy',\n",
       " u'enjoy',\n",
       " u'feel',\n",
       " u'feeling',\n",
       " u'friend',\n",
       " u'gary',\n",
       " u'good',\n",
       " u'horrible',\n",
       " u'im',\n",
       " u'job',\n",
       " u'like',\n",
       " u'love',\n",
       " u'place',\n",
       " u'restaurant',\n",
       " u'sandwich',\n",
       " u'stuff',\n",
       " u'sworn',\n",
       " u'tired',\n",
       " u'today',\n",
       " u'view',\n",
       " u'work']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view features or terms\n",
    "\n",
    "tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'love'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# which term has the vector value 22\n",
    "\n",
    "tfidf_vectorizer.get_feature_names()[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'love', u'restaurant', u'sandwich')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# which terms are vector 22, 24, 25?\n",
    "\n",
    "from operator import itemgetter\n",
    "itemgetter(22,24,25)(tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document-Term Matrix (DTM)\n",
    "\n",
    "This is a matrix view of documents (rows), tokens (columns), and their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.70710678,  0.        ,  0.        ,\n",
       "         0.70710678,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.65674042,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.75411672,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.63033587,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.54894293,  0.        ,\n",
       "         0.        ,  0.        ,  0.54894293,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.70710678,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.70710678],\n",
       "       [ 0.        ,  0.        ,  0.70710678,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.70710678,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.70710678,  0.        ,  0.        ,  0.70710678,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.70710678,  0.        ,  0.70710678,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.70710678,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.70710678,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.70710678,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.70710678,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.75411672,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.65674042,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.70710678,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.70710678,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.5       ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.5       ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.5       ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.5       ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.70710678,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.70710678,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.70710678,  0.70710678,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.57735027,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.57735027,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.57735027,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# document-term matrix using tfidf\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(sms_data).toarray()\n",
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aint</th>\n",
       "      <th>amazing</th>\n",
       "      <th>awesome</th>\n",
       "      <th>beer</th>\n",
       "      <th>beers</th>\n",
       "      <th>believe</th>\n",
       "      <th>best</th>\n",
       "      <th>boss</th>\n",
       "      <th>dandy</th>\n",
       "      <th>deal</th>\n",
       "      <th>...</th>\n",
       "      <th>love</th>\n",
       "      <th>place</th>\n",
       "      <th>restaurant</th>\n",
       "      <th>sandwich</th>\n",
       "      <th>stuff</th>\n",
       "      <th>sworn</th>\n",
       "      <th>tired</th>\n",
       "      <th>today</th>\n",
       "      <th>view</th>\n",
       "      <th>work</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I love this sandwich.</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this is an amazing place!</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.65674</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.754117</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I feel very good about these beers.</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.630336</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this is my best work.</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>what an awesome view</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     aint  amazing   awesome  beer     beers  \\\n",
       "I love this sandwich.                 0.0  0.00000  0.000000   0.0  0.000000   \n",
       "this is an amazing place!             0.0  0.65674  0.000000   0.0  0.000000   \n",
       "I feel very good about these beers.   0.0  0.00000  0.000000   0.0  0.630336   \n",
       "this is my best work.                 0.0  0.00000  0.000000   0.0  0.000000   \n",
       "what an awesome view                  0.0  0.00000  0.707107   0.0  0.000000   \n",
       "\n",
       "                                     believe      best  boss  dandy  deal  \\\n",
       "I love this sandwich.                    0.0  0.000000   0.0    0.0   0.0   \n",
       "this is an amazing place!                0.0  0.000000   0.0    0.0   0.0   \n",
       "I feel very good about these beers.      0.0  0.000000   0.0    0.0   0.0   \n",
       "this is my best work.                    0.0  0.707107   0.0    0.0   0.0   \n",
       "what an awesome view                     0.0  0.000000   0.0    0.0   0.0   \n",
       "\n",
       "                                       ...         love     place  restaurant  \\\n",
       "I love this sandwich.                  ...     0.707107  0.000000         0.0   \n",
       "this is an amazing place!              ...     0.000000  0.754117         0.0   \n",
       "I feel very good about these beers.    ...     0.000000  0.000000         0.0   \n",
       "this is my best work.                  ...     0.000000  0.000000         0.0   \n",
       "what an awesome view                   ...     0.000000  0.000000         0.0   \n",
       "\n",
       "                                     sandwich  stuff  sworn  tired  today  \\\n",
       "I love this sandwich.                0.707107    0.0    0.0    0.0    0.0   \n",
       "this is an amazing place!            0.000000    0.0    0.0    0.0    0.0   \n",
       "I feel very good about these beers.  0.000000    0.0    0.0    0.0    0.0   \n",
       "this is my best work.                0.000000    0.0    0.0    0.0    0.0   \n",
       "what an awesome view                 0.000000    0.0    0.0    0.0    0.0   \n",
       "\n",
       "                                         view      work  \n",
       "I love this sandwich.                0.000000  0.000000  \n",
       "this is an amazing place!            0.000000  0.000000  \n",
       "I feel very good about these beers.  0.000000  0.000000  \n",
       "this is my best work.                0.000000  0.707107  \n",
       "what an awesome view                 0.707107  0.000000  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Document Term Matrix\n",
    "pd.DataFrame(tfidf_matrix,index=sms_data,columns=tfidf_vectorizer.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# do not use this code if your dataset is large ... this will slow down your computer\n",
    "\n",
    "matrix = pd.DataFrame(tfidf_matrix,index=sms_data,columns=tfidf_vectorizer.get_feature_names())\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 Model Building: Training a Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split validation\n",
    "\n",
    "- In Step 4, we built a model using different machine learning (or classification) algorithms. However, it is a **bad idea to evaluate the performance of the model on the same dataset we train the model on**. Thus, we'll use a validation method called ** split validation**.\n",
    "\n",
    "- First, we split the dataset to two parts: **training dataset (70% or 80% of the original dataset)** and **testing dataset (30% or 20% of the original dataset)**. We build a model using training dataset and apply the model to testing dataset and measure the accuracy of the model. You could have a 80-20 split or a 50-50 split.\n",
    "\n",
    "- We will build a predictive model using **x_train** and **y_train**, which are called as **training dataset**.\n",
    "\n",
    "- Then, we will apply the model to **x_test** and **y_test** (**testing dataset**) and this will tell us the performance or quality of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://www.developer.com/imagesvr_ce/6793/ML4.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"http://www.developer.com/imagesvr_ce/6793/ML4.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 12, 4, 4)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(sms_data, sms_labels, test_size=0.2, random_state=0)\n",
    "len(x_train), len(y_train), len(x_test), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13 documents are used to **train** a classification algorithm (or classifier) of our choice; then, the quality of our classification will be **tested** on the other four documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is an amazing place!',\n",
       " 'I am tired of this stuff.',\n",
       " 'he is my sworn enemy!',\n",
       " 'my boss is horrible.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 documents (testing dataset)\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 11)\t0.651520868349\n",
      "  (0, 1)\t0.758630712604\n",
      "  (1, 2)\t0.707106781187\n",
      "  (1, 23)\t0.707106781187\n",
      "+++++++++++++++++++++++++++++++++++++++++\n",
      "  (0, 1)\t1.0\n"
     ]
    }
   ],
   "source": [
    "X_train = tfidf_vectorizer.fit_transform(x_train)\n",
    "X_test = tfidf_vectorizer.transform(x_test)\n",
    "print X_train[:2]\n",
    "print \"+++++++++++++++++++++++++++++++++++++++++\"\n",
    "print X_test[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're working on **classification problem**. There are **different machine learning algorithms** available for building a predictive model\n",
    "\n",
    "<img src =\"http://amueller.github.io/sklearn_tutorial/cheat_sheet.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our features, we can train a classifier to try to predict the category of a post. Let’s start with a naïve Bayes classifier, which provides a nice baseline for this task. scikit-learn includes several variants of this classifier; the one most suitable for word counts is the multinomial variant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "predicted = nb.predict(X_test)\n",
    "np.mean(predicted == y_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['pos', 'pos', 'pos', 'pos'],\n",
       "      dtype='|S3')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pos', 'neg', 'neg', 'neg']\n"
     ]
    }
   ],
   "source": [
    "print y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Model Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " overall accuracy:\n",
      "0.25\n",
      "\n",
      " confusion_matrix:\n",
      "[[0 3]\n",
      " [0 1]]\n",
      "\n",
      " Here is the classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.00      0.00      0.00         3\n",
      "        pos       0.25      1.00      0.40         1\n",
      "\n",
      "avg / total       0.06      0.25      0.10         4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print '\\n overall accuracy:'\n",
    "print metrics.accuracy_score(y_test, predicted)\n",
    "\n",
    "print '\\n confusion_matrix:'\n",
    "print metrics.confusion_matrix(y_test, predicted)\n",
    "\n",
    "print '\\n Here is the classification report:'\n",
    "print metrics.classification_report(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100% classification accuracy ... an ideal classification model ... \n",
    "- 1 positive review is correctly classified as positive review\n",
    "- 3 negative reviews are correctly classificed as negative reviews\n",
    "- zero misclassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/sklearn/utils/deprecation.py:70: DeprecationWarning: Function plot_confusion_matrix is deprecated; This will be removed in v0.4.0. Please use scikitplot.metrics.plot_confusion_matrix instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUIAAAEWCAYAAAATsp59AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlvf/AW1UnJlN6OxkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1085621d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# optional\n",
    "# using scikit-plot python package (https://github.com/reiinakano/scikit-plot) ... you need to install \n",
    "# !pip install scikit-plot\n",
    "\n",
    "from scikitplot import plotters as skplt\n",
    "\n",
    "skplt.plot_confusion_matrix(y_true=np.array(y_test), y_pred=predicted)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to interpret the results (important !!!)\n",
    "\n",
    "Assume there are 138 customer reviews. **68 reviews are positive (0). 70 reviews are negative (1)**. \n",
    "\n",
    "Consider this result:\n",
    "\n",
    "**[[65, 5]** \n",
    "\n",
    "**[10, 58]]**\n",
    " \n",
    " - Overall accuracy = **(65 + 58) / 138 = 0.89**   \n",
    " - 65 positive reviews classified as positive\n",
    " - 58 correctly as negative reviews\n",
    " - 5 positive reviews as negative (misclassification) - this is called false positive\n",
    " - 10 negative reviews as positive reviews (misclassification) - false negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. Model Deployment\n",
    "\n",
    "<img src=\"http://www.nltk.org/images/supervised-classification.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To try to predict the outcome on a new document we need to extract the features using almost the same feature extracting chain as before. The difference is that we call transform instead of fit_transform on the transformers, since they have already been fit to the training set:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# reading unlabeled data for classification\n",
    "\n",
    "docs_new = []\n",
    "\n",
    "openfile = open('data/unlabeled_sampledata.csv', 'rb')\n",
    "\n",
    "r = csv.reader(openfile)\n",
    "\n",
    "for i in r:\n",
    "    docs_new.append(i[0])\n",
    "    \n",
    "openfile.close()\n",
    "\n",
    "docs_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'I really love donuts' => pos\n",
      "'New York is an amazing place to visit.' => pos\n",
      "'I am so tired of this food, bad' => pos\n",
      "\"I can't deal with him any more. He is so mean, bad.\" => neg\n"
     ]
    }
   ],
   "source": [
    "docs_new = [\"I really love donuts\",\n",
    "            \"New York is an amazing place to visit.\",\n",
    "            \"I am so tired of this food, bad\",\n",
    "            \"I can't deal with him any more. He is so mean, bad.\"]\n",
    "\n",
    "unlabeled_tfidf = tfidf_vectorizer.transform(docs_new)\n",
    "\n",
    "predicted = nb.predict(unlabeled_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'I really love donuts' => [ 0.31148283  0.68851717]\n",
      "'New York is an amazing place to visit.' => [ 0.30514172  0.69485828]\n",
      "'I am so tired of this food, bad' => [ 0.41666667  0.58333333]\n",
      "\"I can't deal with him any more. He is so mean, bad.\" => [ 0.60700762  0.39299238]\n"
     ]
    }
   ],
   "source": [
    "#let's find the probability\n",
    "\n",
    "predicted_prob = nb.predict_proba(unlabeled_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted_prob):\n",
    "    print('%r => %s' % (doc, category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 19)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (3, 8)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print unlabeled_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'restaurant', u'like', u'amazing', u'job', u'beers')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemgetter(20,18,1,17,4)(tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "- Well handles a large number of variables\n",
    "- Variables treated as independent (“naïve”), and the relationship between them are not calculated\n",
    "- Bayesian theorem (probability)\n",
    "<img src = \"images\\naive_1.gif\">\n",
    "<img src = \"images\\naive_2.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everything in one cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1. we're importing the data ... \n",
      "Step 2. text preprocessing ... \n",
      "Step 3. Split validation & Building a predictive text classification model ... \n",
      "Step 4. Model evaluation ... \n",
      "     The model accuracy is: \n",
      "0.25\n",
      "step 5. Model deployment & application ...\n",
      "'I really love donuts' => pos\n",
      "'New York is an amazing place to visit.' => pos\n",
      "'I am so tired of this food' => pos\n",
      "\"I can't deal with him any more. He is so mean, bad.\" => neg\n"
     ]
    }
   ],
   "source": [
    "#Step 1. Import the data\n",
    "\n",
    "print \"Step 1. we're importing the data ... \"\n",
    "\n",
    "# movie review in the first column\n",
    "sms_data = []\n",
    "# sentiment in the second colummn\n",
    "sms_labels = []\n",
    "#both columns\n",
    "sms = []\n",
    "\n",
    "openfile = open('data/sampledata_classification.csv', 'rb')\n",
    "r = csv.reader(openfile)\n",
    "for i in r:\n",
    "    sms.append(i)\n",
    "    sms_data.append(i[0])\n",
    "    sms_labels.append(i[1])    \n",
    "openfile.close()\n",
    "\n",
    "#Step 2. Text preprocessing\n",
    "print \"Step 2. text preprocessing ... \"\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(decode_error ='ignore', stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(sms_data)\n",
    "\n",
    "#Step 3. Building a predictive text classification model \n",
    "print \"Step 3. Split validation & Building a predictive text classification model ... \"\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(sms_data, sms_labels, test_size=0.2, random_state=0)\n",
    "len(x_train), len(y_train), len(x_test), len(y_test)\n",
    "\n",
    "X_train = tfidf_vectorizer.fit_transform(x_train)\n",
    "X_test = tfidf_vectorizer.transform(x_test)\n",
    "\n",
    "nb = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "predicted = nb.predict(X_test)\n",
    "np.mean(predicted == y_test)  \n",
    "#Naive Bayes algorithm predicts as following\n",
    "\n",
    "#Step 4. Model evaluation\n",
    "print \"Step 4. Model evaluation ... \"\n",
    "predicted = nb.predict(X_test)\n",
    "\n",
    "#compare the predictec value with the real values\n",
    "print \"     The model accuracy is: \"\n",
    "print np.mean(predicted == y_test) \n",
    "\n",
    "#Step 5. Model deployment & application\n",
    "print \"step 5. Model deployment & application ...\"\n",
    "docs_new = [\"I really love donuts\",\n",
    "            \"New York is an amazing place to visit.\",\n",
    "            \"I am so tired of this food\",\n",
    "            \"I can't deal with him any more. He is so mean, bad.\"]\n",
    "\n",
    "unlabeled_tfidf = tfidf_vectorizer.transform(docs_new)\n",
    "\n",
    "predicted = nb.predict(unlabeled_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, category))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to get most informative features for scikit-learn classifier for different class?\n",
    "https://stackoverflow.com/questions/26976362/how-to-get-most-informative-features-for-scikit-learn-classifier-for-different-c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "for i in feature_names[:10]:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/26976362/how-to-get-most-informative-features-for-scikit-learn-classifier-for-different-c\n",
    "\n",
    "def most_informative_feature_for_binanry_classification(vectorizer, classifier, n=50):\n",
    "    \n",
    "    class_labels = classifier.classes_\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    topn_class1 = sorted(zip(classifier.coef_[0], feature_names))[:n]\n",
    "    topn_class2 = sorted(zip(classifier.coef_[0], feature_names))[-n:]\n",
    "\n",
    "    for coef, feat in topn_class1:\n",
    "        print(class_labels[0], coef, feat)\n",
    "\n",
    "    print \"=========================================\"\n",
    "\n",
    "    for coef, feat in reversed(topn_class2):\n",
    "        print(class_labels[1], coef, feat)\n",
    "\n",
    "\n",
    "most_informative_feature_for_binanry_classification(tfidf_vectorizer, nb, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coef_ attribute of MultinomialNB is a re-parameterization of the naive Bayes model as a linear classifier model. For a binary classification problems this is basically the log of the estimated probability of a feature given the positive class. It means that \n",
    "\n",
    "- **higher values mean more important features for the positive class**\n",
    "- **lower values mean more information features for the negative class**\n",
    "\n",
    "The above print shows the \n",
    "- top 10 lowest values (most important features) for the negative class\n",
    "- top 10 higest values (most important features) for the positive class\n",
    "\n",
    "https://stackoverflow.com/questions/29867367/sklearn-multinomial-nb-most-informative-features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "- http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html (Naive Bayes)\n",
    "- https://stackoverflow.com/questions/26976362/how-to-get-most-informative-features-for-scikit-learn-classifier-for-different-c\n",
    "- http://www.datasciencecentral.com/profiles/blogs/5-text-classification-case-studies-using-scikit-learn\n",
    "- http://blog.paralleldots.com/text-analytics/text-classification-applications-use-cases/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
