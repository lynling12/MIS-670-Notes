{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Regression Techniques \n",
    "\n",
    "While multiple regression is better than univariate regression because the former finds a model solution with **lower SSE** (higher r-squared). But, if there are **too many X variables (e.g., > 10) in a regression model**, the model becomes **too complex (or overfitting) to be useful** in practice due to **multicollinearity and difficulity of interpretation**. This problem is also known as **\"curse of high dimensionality\"**\n",
    "\n",
    "Thus, **more advanced regression would be needed to deal with this issue**\n",
    "\n",
    "> ## 1. Regularization \n",
    "> - refers to **the process of penalizing the model with too many redundant variables (or highly correlated variables)**\n",
    "> - the goal is developing the regression model with **low SSE** and **simplicity (fewer X variables or predictors)** \n",
    "> - **two types of regression include regulariation in their objective function: Lasso and Ridge**\n",
    "> - http://scikit-learn.org/stable/modules/linear_model.html\n",
    "\n",
    "> ## 2. Feature selection\n",
    "> - refers to **the process of selecting the most useful predictors**, helping analysts understand what predictors matter in predicting y value\n",
    "> - the goal is developing the **simple** regression model with the **user specificed number of predictors**.\n",
    "> - f_regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso regression (Regularization)\n",
    "\n",
    "- (Least Absolute Shrinkage and Selection Operator) is one of the **regression models with regularization**\n",
    "- Finds the model solution with **fewer X variables**\n",
    "\n",
    "> #### How does Lasso work?#### \n",
    "\n",
    "> - Remember that the goal of regression is **Minimize SSE** and more predictors is likely to reduce **SSE**\n",
    "> - Thus, Lasso includes **regularization** or **a mechanism of penalizing adding too many variables**. \n",
    "\n",
    "                  minimize (SSE  + alpha|coefficient|)\n",
    "\n",
    "                  where alpha = parameter for penalizing adding more coefficients\n",
    "\n",
    "> - This approach reinforces the Lasso regression to consider fewer predictors (simpler regression model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#regression packages\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import explained_variance_score\n",
    "import statsmodels.formula.api as sm\n",
    "\n",
    "#lasso regression\n",
    "from sklearn import linear_model\n",
    "\n",
    "#f_regression (feature selection)\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# recursive feature selection (feature selection)\n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = pd.read_csv(\"data/baseball.csv\")\n",
    "teams.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Below is an explanation of the teams DataFrame attribtues.\n",
    "\n",
    "yearID: Year\n",
    "teamID: Team\n",
    "Rank: Position in final standings\n",
    "R: Runs scored\n",
    "RA: Opponents runs scored\n",
    "G: Games played\n",
    "W: Wins\n",
    "H: Hits by batters\n",
    "BB: Walks by batters\n",
    "HBP: Batters hit by pitch\n",
    "AB: At bats\n",
    "SF: Sacrifice flies\n",
    "HR: Homeruns by batters\n",
    "2B: Doubles\n",
    "3B: Triples\n",
    "Batting Average (BA) = H/AB    \n",
    "On Base Percentage (OBP) = (H+BB+HBP)/(AB+BB+HBP+SF)\n",
    "Slugging Percentage (SLG) = H+2B+(2*3B)+(3*HR)/AB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On Base Percentage (OBP, On Base Average, OBA) is a measure of how often a batter reaches base. \n",
    "\n",
    "The full formula is OBP = (Hits + Walks + Hit by Pitch) / (At Bats + Walks + Hit by Pitch + Sacrifice Flies). Batters are not credited with reaching base on an error or fielder's choice, and they are not charged with an opportunity if they make a sacrifice bunt.\n",
    "\n",
    "All Time Leaders\n",
    "Ted Williams\t.482\t(career)\n",
    "Barry Bonds\t    .609\t(2004 season)\n",
    "\n",
    "http://www.baseball-reference.com/bullpen/On_base_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = teams.drop(['yearID', 'teamID', 'Rank'], axis=1)\n",
    "teams.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#assigning columns to X and Y variables\n",
    "y = teams['R'] \n",
    "X = teams.drop(['R'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the model\n",
    "model1 = linear_model.Lasso(alpha=1)         #higher alpha (penality parameter), fewer predictors\n",
    "model1.fit(X, y)\n",
    "model1_y = model1.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Coefficients: ', model1.coef_\n",
    "print \"y-intercept \", model1.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = [\"%.3f\" % i for i in model1.coef_]\n",
    "xcolumns = [ i for i in X.columns ]\n",
    "zip(xcolumns, coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression model has become a lot simpler than the full model with all X variables. Several X variables were removed from the model, including **G, AB, salary, BA, OBP, and SLG**. These removed variables have their **coefficients close to 0**.\n",
    "\n",
    "R = 0.212RA + 2.113W + 0.471H + 0.242BB + ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"mean square error: \", mean_squared_error(y, model1_y)\n",
    "print \"variance or r-squared: \", explained_variance_score(y, model1_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# f_Regression (Feature Selection)\n",
    "\n",
    "- Quick linear model for testing the effect of a single predictor, sequentially for many predictors.\n",
    "- http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html#sklearn.feature_selection.f_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selec only 2 X variables\n",
    "X_new = SelectKBest(f_regression, k=2).fit_transform(X, y)\n",
    "X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f_regression determines that **OBP** and **SLG** are two most important predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = lm.LinearRegression()\n",
    "model2.fit(X_new, y)\n",
    "model2_y = model2.predict(X_new)\n",
    "\n",
    "print \"mean square error: \", mean_squared_error(y, model2_y)\n",
    "print \"variance or r-squared: \", explained_variance_score(y, model2_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use f_regression with k = 3 and develop a new regression model\n",
    "\n",
    "\n",
    "model3\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Feature Selection (RFE): Another Feature Selection Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = lm.LinearRegression()\n",
    "rfe = RFE(lr, n_features_to_select=2)\n",
    "rfe_y = rfe.fit(X,y)\n",
    "\n",
    "print \"Features sorted by their rank:\"\n",
    "print sorted(zip(map(lambda x: x, rfe.ranking_), X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or you can do something like this: zip first and sort the data\n",
    "\n",
    "print sorted(zip(rfe.ranking_, X.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: with fewer predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#First Model\n",
    "runs_reg_model1 = sm.ols(\"R~OBP+SLG+BA\",teams)\n",
    "runs_reg1 = runs_reg_model1.fit()\n",
    "#Second Model\n",
    "runs_reg_model2 = sm.ols(\"R~OBP+SLG\",teams)\n",
    "runs_reg2 = runs_reg_model2.fit()\n",
    "#Third Model\n",
    "runs_reg_model3 = sm.ols(\"R~BA\",teams)\n",
    "runs_reg3 = runs_reg_model3.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first one will have as features OBP, SLG and BA. \n",
    "- The second model will have as features OBP and SLG. \n",
    "- The last one will have as feature BA only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print runs_reg1.summary()\n",
    "print runs_reg2.summary()\n",
    "print runs_reg3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first model has an Adjusted R-squared of 0.918, with 95% confidence interval of BA between -283 and 468. This is counterintuitive, since we expect the BA value to be positive. This is due to a **multicollinearity** between the variables.\n",
    "\n",
    "- The second model has an Adjusted R-squared of 0.919, and the last model an Adjusted R-squared of 0.500.\n",
    "\n",
    "- Based on this analysis, we could confirm that the second model using **OBP** and **SLG** is the best model for predicting Run Scored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- http://adilmoujahid.com/posts/2014/07/baseball-analytics/ (reproduced from this page)\n",
    "- https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/#four\n",
    "- http://www.python-course.eu/lambda.php (excellent resource for lamda and map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
