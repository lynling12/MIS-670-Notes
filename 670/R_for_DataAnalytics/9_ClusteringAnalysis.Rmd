---
title: "Clustering Analysis"
---

```{r}
#find out your current directory
getwd()   # document would be the right directory.

#change the current directory to 665 or 670 depending on the course you're taking, if needed.
setwd("C:/Users/bchae/Documents/R/665")
# setwd("C:/Users/bchae/Documents/R/670")

# check again ... just make sure the working directory is C:/Users/yourloginID/Documents/R/665 
getwd()
```


```{r}
df <- read.csv("Chapter06DataSet.csv")
head(df)
```

```{r}
df <- scale(df) # standardize variables
head(df)
```

### Optimal k value
http://www.statmethods.net/advstats/cluster.html
```{r}
# Determine number of clusters
wss <- (nrow(df)-1)*sum(apply(df,2,var))
for (i in 2:15) wss[i] <- sum(kmeans(df, centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters",
  ylab="Within groups sum of squares")
```

### Kmeans

```{r}
# K-Means Cluster Analysis
fit <- kmeans(df, 4) # 5 cluster solution
# get cluster means 
aggregate(df,by=list(fit$cluster),FUN=mean)
# append cluster assignment
df <- data.frame(df, fit$cluster)
head(df)
```

### Hierarhical Agglomerative
```{r fig.height=10, fig.width=20}
# Ward Hierarchical Clustering
d <- dist(df, method = "euclidean") # distance matrix
fit <- hclust(d, method="ward.D2") 
plot(fit) # display dendogram
groups <- cutree(fit, k=5) # cut tree into 5 clusters
# draw dendogram with red borders around the 5 clusters 
rect.hclust(fit, k=5, border="red")
```


